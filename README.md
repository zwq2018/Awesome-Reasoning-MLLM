# Awesome-Reasoning-MLLM

ğŸ‘ Welcome to the Awesome-Reasoning-MLLM repository! This repository is a curated collection of the most influential papers, code, dataset, benchmarks, and resources about Reasoning in Multi-Modal Large Language Models (MLLMs) and Vision-Language Models (VLMs).

Feel free to â­ star and fork this repository to keep up with the latest advancements and contribute to the community.


## ğŸ“’ Table of Contents
- [Awesome-Reasoning-MLLM](#awesome-reasoning-mllm)
  - [Post-Training / Reinforcement Learning](#reinforcement-learning)
  - [Tree Search / MCTS](#mcts--tree-search)
  - [SFT / CoT](#sft--cot)
  - [Data](#data)

## Post-Training / Reinforcement Learning
* [Vision-R1, 2503] Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models [[PaperğŸ“‘]](https://arxiv.org/abs/2503.06749) [[CodeğŸ”§]](https://arxiv.org/abs/2503.06749)
* [VisualThinker-R1-Zero, 2503] R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model [[PaperğŸ“‘]](https://arxiv.org/pdf/2503.05132) [[CodeğŸ”§]](https://github.com/turningpoint-ai/VisualThinker-R1-Zero)
* [MM-Eureka, 2503] MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning [[PaperğŸ“‘]](https://arxiv.org/abs/2503.07365) [[CodeğŸ”§]](https://github.com/ModalMinds/MM-EUREKA)
* [Visual-RFT, 2503] Visual-RFT: Visual Reinforcement Fine-Tuning [[PaperğŸ“‘]](https://arxiv.org/abs/2503.01785) [[CodeğŸ”§]](https://github.com/Liuziyu77/Visual-RFT) 
* [R1-V, 2502] R1-V: Reinforcing Super Generalization Ability in Vision Language Models with Less Than $3 [[CodeğŸ”§]](https://github.com/Deep-Agent/R1-V)

## Tree Search / MCTS
* [AStar, 2502] Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking [[PaperğŸ“‘]](https://arxiv.org/abs/2502.02339) [[CodeğŸ”§]](https://github.com/Aurora-slz/MM-Verify)
* [Mulberry, 2412] Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search [[PaperğŸ“‘]](https://arxiv.org/abs/2412.18319) [[CodeğŸ”§]](https://github.com/HJYao00/Mulberry) 

<!--## Test-time Reasoning-->

##  SFT / CoT
* [MM-Verify, 2502] MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought Verification [[PaperğŸ“‘]](https://www.arxiv.org/abs/2502.13383)
* [LlamaV-o1, 2501] LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs [[PaperğŸ“‘]](https://arxiv.org/abs/2501.06186)
* [LLaVA-CoT, 2411] LLaVA-CoT: Let Vision Language Models Reason Step-by-Step [[PaperğŸ“‘]](https://arxiv.org/abs/2411.10440) [[CodeğŸ”§]](https://github.com/PKU-YuanGroup/LLaVA-CoT)

## Data
* [Mulberry 260K SFT, 2412] Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search [[PaperğŸ“‘]](https://arxiv.org/abs/2412.18319) [[CodeğŸ”§]](https://github.com/HJYao00/Mulberry) 
* [LLaVA-CoT 100K SFT, 2411] LLaVA-CoT: Let Vision Language Models Reason Step-by-Step [[PaperğŸ“‘]](https://arxiv.org/abs/2411.10440) [[CodeğŸ”§]](https://github.com/PKU-YuanGroup/LLaVA-CoT)

<!--## Benchmark-->
