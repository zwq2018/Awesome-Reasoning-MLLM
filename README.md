# Awesome-Reasoning-MLLM

👏 Welcome to the Awesome-Reasoning-MLLM repository! This repository is a curated collection of the most influential papers, code, dataset, benchmarks, and resources about Reasoning in Multi-Modal Large Language Models (MLLMs) and Vision-Language Models (VLMs).

Feel free to ⭐ star and fork this repository to keep up with the latest advancements and contribute to the community.


## 📒 Table of Contents
- [Awesome-Reasoning-MLLM](#awesome-reasoning-mllm)
  - [Post-Training / Reinforcement Learning](#post-training--reinforcement-learning)
  - [Tree Search / MCTS](#tree-search--mcts)
  - [SFT / CoT](#sft--cot)
  - [Data](#data)

## Post-Training / Reinforcement Learning
* [LMM-R1, 2503] LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities [[Paper📑]](https://arxiv.org/pdf/2503.07536)[[Code🔧]](https://github.com/TideDra/lmm-r1)
* [R1-VL, 2503] R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization [[Paper📑]](https://arxiv.org/abs/2503.12937)[[Code🔧]](https://github.com/jingyi0000/R1-VL)
* [Vision-R1, 2503] Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models [[Paper📑]](https://arxiv.org/abs/2503.06749) [[Code🔧]](https://arxiv.org/abs/2503.06749)
* [VisualThinker-R1-Zero, 2503] R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model [[Paper📑]](https://arxiv.org/pdf/2503.05132) [[Code🔧]](https://github.com/turningpoint-ai/VisualThinker-R1-Zero)
* [MM-Eureka, 2503] MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning [[Paper📑]](https://arxiv.org/abs/2503.07365) [[Code🔧]](https://github.com/ModalMinds/MM-EUREKA)
* [Visual-RFT, 2503] Visual-RFT: Visual Reinforcement Fine-Tuning [[Paper📑]](https://arxiv.org/abs/2503.01785) [[Code🔧]](https://github.com/Liuziyu77/Visual-RFT)
* [VLM-R1] VLM-R1: A stable and generalizable R1-style Large Vision-Language Model [[Code🔧]](https://github.com/om-ai-lab/VLM-R1/tree/main?tab=readme-ov-file)
* [R1-V, 2502] R1-V: Reinforcing Super Generalization Ability in Vision Language Models with Less Than $3 [[Code🔧]](https://github.com/Deep-Agent/R1-V)


## Tree Search / MCTS
* [AStar, 2502] Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking [[Paper📑]](https://arxiv.org/abs/2502.02339)
* [Mulberry, 2412] Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search [[Paper📑]](https://arxiv.org/abs/2412.18319) [[Code🔧]](https://github.com/HJYao00/Mulberry) 

<!--## Test-time Reasoning-->

##  SFT / CoT
* [Embodied Reasoner, 2503] Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks [[Paper📑]](https://arxiv.org/abs/2503.21696) [[Code🔧]](https://github.com/zwq2018/embodied_reasoner)
* [MM-Verify, 2502] MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought Verification [[Paper📑]](https://www.arxiv.org/abs/2502.13383) [[Code🔧]](https://github.com/Aurora-slz/MM-Verify)
* [LlamaV-o1, 2501] LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs [[Paper📑]](https://arxiv.org/abs/2501.06186) [[Code🔧]](https://github.com/mbzuai-oryx/LlamaV-o1)
* [Insight-V, 2411] Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models [[Paper📑]](https://arxiv.org/abs/2411.14432) [[Code🔧]](https://github.com/dongyh20/Insight-V)
* [LLaVA-CoT, 2411] LLaVA-CoT: Let Vision Language Models Reason Step-by-Step [[Paper📑]](https://arxiv.org/abs/2411.10440) [[Code🔧]](https://github.com/PKU-YuanGroup/LLaVA-CoT)
* [LLaVA-Reasoner, 2410] Improve Vision Language Model Chain-of-thought Reasoning [[Paper📑]](https://arxiv.org/abs/2410.16198) [[Code🔧]](https://github.com/RifleZhang/LLaVA-Reasoner-DPO)
* [Visual-CoT, 2403] Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning [[Paper📑]](https://arxiv.org/abs/2403.16999) [[Code🔧]](https://github.com/deepcs233/Visual-CoT)

## Data
* [Mulberry 260K SFT, 2412] Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search [[Paper📑]](https://arxiv.org/abs/2412.18319) [[Code🔧]](https://github.com/HJYao00/Mulberry) 
* [LLaVA-CoT 100K SFT, 2411] LLaVA-CoT: Let Vision Language Models Reason Step-by-Step [[Paper📑]](https://arxiv.org/abs/2411.10440) [[Code🔧]](https://github.com/PKU-YuanGroup/LLaVA-CoT)

<!--## Benchmark-->
