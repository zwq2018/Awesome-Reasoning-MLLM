# Awesome-Reasoning-MLLM

ğŸ‘ Welcome to the Awesome-Reasoning-MLLM repository! This repository is a curated collection of the most influential papers, code, dataset, benchmarks, and resources about Reasoning in Multi-Modal Large Language Models (MLLMs) and Vision-Language Models (VLMs).

Feel free to â­ star and fork this repository to keep up with the latest advancements and contribute to the community.


## ğŸ“’ Table of Contents

## Reinforcement Learning

## MCTS/Tree Search
* [AStar] Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking [Paper](https://arxiv.org/abs/2502.02339)
* [Mulberry] Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search [[PaperğŸ“‘]](https://arxiv.org/abs/2412.18319) [[CodeğŸ”§]](https://github.com/HJYao00/Mulberry) 

## Test-time Reasoning

## CoT
* [LLaVA-CoT] [LLaVA-CoT: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/abs/2411.10440)

## Data
* [Mulberry 260K SFT] Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search [[PaperğŸ“‘]](https://arxiv.org/abs/2412.18319) [[CodeğŸ”§]](https://github.com/HJYao00/Mulberry) 
* [LLaVA-CoT 100K SFT] LLaVA-CoT: Let Vision Language Models Reason Step-by-Step [[PaperğŸ“‘]](https://arxiv.org/abs/2411.10440) [[CodeğŸ”§]](https://github.com/PKU-YuanGroup/LLaVA-CoT)

## Benchmark
